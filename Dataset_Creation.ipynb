{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook creates the dataset from the Syllabi File. Please visit the README file to learn the naming convention for the syllabi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necassary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install textract\n",
    "%pip install pdf2docx\n",
    "%pip install pyChatGPT\n",
    "%pip install openai;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import csv\n",
    "from pdf2docx import Converter\n",
    "import textract\n",
    "import re\n",
    "import string \n",
    "import docx2txt\n",
    "from pyChatGPT import ChatGPT\n",
    "import time\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to the syllabi folder and print current files in Syllabi Directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate and Get file names out of directory\n",
    "Path = \"Syllibi/\"\n",
    "DirList = os.listdir(Path)\n",
    "#DirList.remove('.DS_Store')\n",
    "print(DirList)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all special characters function used to clean up the passages on they are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpecialCharacter(s):\n",
    "    t = \"\"\n",
    "    la=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "    ua=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    nu=\"0123456789\"\n",
    "    space = ' '\n",
    "    endline = '\\n\\r\\t'\n",
    "    punc=string.punctuation\n",
    "    for i in s:\n",
    "        if(i in la or i in ua or i in punc or i in nu or i in space or i in endline):\n",
    "            t+=i\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtractTitle function is used to parse through the naming convention and assign the information to variables. These features are added to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracttitle(Filetitle):\n",
    "    Extractedlist = Filetitle.split(\",\")\n",
    "    print(Extractedlist)\n",
    "    if (len(Extractedlist) != 5):\n",
    "        return 0\n",
    "    classtype = Extractedlist[0]\n",
    "    classnumber = Extractedlist[1]\n",
    "    classtitle = Extractedlist[2]\n",
    "    year = Extractedlist[3]\n",
    "    professor = Extractedlist[4]\n",
    "    professor = professor[:-5]\n",
    "    print(classtype, classnumber, classtitle, year, professor)\n",
    "    return classtype, classnumber, classtitle, professor, year\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with third party Chat GPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#session_token = \"\"\n",
    "#api = ChatGPT(auth_type='google', email='', password='')\n",
    "#api = ChatGPT(session_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign openai api key for text-divinci-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\"\n",
    "openai.Model.retrieve(\"text-davinci-003\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the files to docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,len(DirList)):\n",
    "    if \".pdf\" in DirList[x]:\n",
    "        tempe = DirList[x]\n",
    "        path_input = Path + DirList[x]\n",
    "        path_output = Path + tempe[:-4] + '.docx'\n",
    "        cv = Converter(path_input)\n",
    "        cv.convert(path_output, start=0, end=None)\n",
    "        cv.close()\n",
    "        os.remove(path_input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine File List\n",
    "DirList = os.listdir(Path)\n",
    "counter = 0\n",
    "file = open('SyllabiDataset.csv', 'w', newline ='')\n",
    "\n",
    "for x in range(0,len(DirList)):\n",
    "    counter += 1\n",
    "\n",
    "    #Make sure the file is in .docx form\n",
    "    if \".docx\" in DirList[x]:\n",
    "\n",
    "        #extract dataset information from syllabis title\n",
    "        department, number, name, professor, year = extracttitle(DirList[x])\n",
    "\n",
    "        # Create the Path to the Doc File\n",
    "        temp = ''\n",
    "        tempdoc = Path + DirList[x]\n",
    "        \n",
    "        # extract the text then convert byte object to string\n",
    "        text = docx2txt.process(tempdoc)\n",
    "        text = removeSpecialCharacter(text)\n",
    "        \n",
    "        #Split by tab and enter\n",
    "        Splittext = re.split('\\n|\\t',text)\n",
    "        \n",
    "        #remove empty strings and then remove unwanted spaces\n",
    "        while(\"\" in Splittext):\n",
    "            Splittext.remove('')\n",
    "        for p in range(0,len(Splittext)):\n",
    "            Splittext[p] = re.sub(\"\\s\\s+\" , \" \", Splittext[p])\n",
    "       \n",
    "        #Use 1000 character threshold to combine strings.\n",
    "        NewList = []\n",
    "        NewList.append(Splittext[0])\n",
    "        for y in range(1, len(Splittext)):\n",
    "            if (len(NewList[-1]) < 1000):\n",
    "                NewList[-1] = (NewList[-1] + ' ' + Splittext[y])\n",
    "            else:\n",
    "                NewList.append(Splittext[y])\n",
    "        \n",
    "        # writing the data into the csv file\n",
    "        file = open('SyllabiDataset.csv', 'a', newline ='') \n",
    "        \n",
    "        #open file\n",
    "        with file:\n",
    "\n",
    "            #Go through dataset and create 3 question,answer, and keyword pairs using openai text-divinci-003\n",
    "            for x in range(1,len(NewList)):\n",
    "\n",
    "                #Send API request\n",
    "                prom = \"Create 3 questions that each have a short answer and 3 to 5 keywords in the format 'Question: >> Answer: >> keywords:'. Dont apply any formatting in the response only follow the exact format stated do not split Question: >> Answer: >> keywords:. base the question on this text: \" + NewList[x]\n",
    "                resp = openai.Completion.create(\n",
    "                    model=\"text-davinci-003\",\n",
    "                    prompt=prom,\n",
    "                    max_tokens=3000\n",
    "                        )\n",
    "                \n",
    "                #extract response from API call\n",
    "                QA = resp['choices'][0]['text']\n",
    "                #split by new line to get the pairs\n",
    "                QAnew = re.split('\\n',QA)\n",
    "                while(\"\" in QAnew):\n",
    "                    QAnew.remove('')\n",
    "                while(' ' in QAnew):\n",
    "                    QAnew.remove(' ')\n",
    "\n",
    "                #Check to make sure the paragraphs have all three pairs of Question and Answer\n",
    "                for elements in QAnew:\n",
    "                    if \"QA1\" in elements:\n",
    "                        pass\n",
    "                    elif \"QA2\" in elements:\n",
    "                        pass\n",
    "                    elif \"QA3\" in elements:\n",
    "                        pass\n",
    "                    elif \"Q\" in elements:\n",
    "                        pass\n",
    "                    elif \"Q1.\" in elements:\n",
    "                        pass\n",
    "                    elif \"Q2\" in elements:\n",
    "                        pass\n",
    "                    elif \"Q3\" in elements:\n",
    "                        pass\n",
    "                    elif \"Question.\" in elements:\n",
    "                        pass\n",
    "                    elif \"Question:\" in elements:\n",
    "                        pass\n",
    "                    elif \"Question 1:\" in elements:\n",
    "                        pass\n",
    "                    elif \"Question 2:\" in elements:\n",
    "                        pass\n",
    "                    elif \"Question 3:\" in elements:\n",
    "                        pass\n",
    "                    else:\n",
    "                        QAnew.remove(elements)\n",
    "                print(QAnew)\n",
    "                if(len(QAnew) != 3):\n",
    "                    continue\n",
    "                QA1 = re.split('>>', QAnew[0])\n",
    "                if(len(QA1) != 3):\n",
    "                    continue \n",
    "                QA2 = re.split('>>', QAnew[1])\n",
    "                if(len(QA2) != 3):\n",
    "                    continue \n",
    "                QA3 = re.split('>>', QAnew[2])\n",
    "                if(len(QA3) != 3):\n",
    "                    continue\n",
    "                    \n",
    "                #API request limit per minute so sleep for 2 seconds to prevent API request failure\n",
    "                time.sleep(2)\n",
    "                    # QA4 = re.split('>', QAnew[3])\n",
    "                    # print(len(QA4))\n",
    "                    # QA5 = re.split('>', QA[4])    \n",
    "                    #api.refresh_auth()\n",
    "                    #api.clear_conversations()\n",
    "\n",
    "                #Write the question/answer/keywords to corresponding passage entries\n",
    "                improvemen = \"This class is \"+department+ ' '+ number+ ', '+ name+ ', taught by '+professor+ ' in '+ year+'. '  \n",
    "                fieldnames = ['text', 'class type', 'class number','class name', 'class professor', 'class year', 'Q1', 'A1', 'K1', 'Q2', 'A2', 'K2', 'Q3', 'A3', 'K3']#, 'Q4', 'A4', 'K4']#, 'Q5', 'A5', 'K5']\n",
    "                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                writer.writerow({'text':improvemen + NewList[x], 'class type': department, 'class number': number,'class name': name, 'class year': year, 'class professor': professor , 'Q1': QA1[0], 'A1': QA1[1], 'K1': QA1[2], 'Q2': QA2[0], 'A2': QA2[1], 'K2': QA2[2], 'Q3': QA3[0], 'A3': QA3[1], 'K3': QA3[2]})#, 'Q4': QA4[0], 'A4': QA4[1], 'K4': QA4[2]})#, 'Q5': QA5[0], 'A5': QA5[1], 'K5': QA5[2]})\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7acf0e7c55703b1d62125093779d76af3b1e04738383ec7b3594d323472f8b38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
